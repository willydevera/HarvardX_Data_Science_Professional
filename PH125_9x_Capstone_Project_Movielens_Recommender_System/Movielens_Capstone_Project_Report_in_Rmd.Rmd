---
title: 'HarvardX: PH125.9x Data Science: Capstone Project'
subtitle: 'Movielens Recommender System Project Report'
author: "Wilfredo A. de Vera"
date: "June 12, 2020"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
  number_sections: true
  toc: true
  keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = FALSE
)
options(tinytex.verbose = TRUE)
```

##############

```{r harvardx_data_load, message=FALSE, warning=FALSE, include=FALSE}

################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

# if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
# if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
# if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

# load libraries
library(tinytex)
library(tidyverse)
library(caret)
library(ggplot2)
library(knitr)
library(kableExtra)


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")


# Validation set will be 10% of MovieLens data
# set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index, ]
temp <- movielens[test_index, ]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# remove temporary files
rm(dl,ratings,movies, test_index, temp,movielens,removed)

###################

```


####################################

## 1. Introduction

The objective of this project is to build recommender system models using the movielens dataset in partial fulfilment of the Harvardx Data Science Capstone course PH125.9x.

Subsequent to the wrangling and cleaning of the edx and validation sets, models were developed and trained using selected variables from the train_set, which comprises 70% of the wrangled edx dataset.  These were then tested on the test_set (comprising 30% of wrangled edx) and eventually on the validation set, with the following dimensions indicated below:

train_set: 16,359,990 obs. 11 variables

test_set :  7,011,381 obs. 11 variables

validation: 2,595,763 obs. 11 variables


The goal is to determine the algorithm that yields the least root mean squared error (RMSE) following the equation below:

$$\mbox{RMSE} = \sqrt{\frac{1}{n}\sum_{t=1}^{n}(\hat{y}_{u,i} - y_{u,i})^2}$$

Eighteen models were built and the RMSE assessed on both the test and validation sets. These models were similar to those indicated in Section 33.7 of the Introduction to Data Science Book (https://rafalab.github.io/dsbook/); albeit, with the addition of other independent variables such as genres, weekday_rated, year_released, and year_rated. Note that these are aside from userId and movieId already used as variables in the book.  In addition, stats's linear regression (lm) model was included.  Furthermore and out of curiousity,  random forests, generalized linear model (glm), deep neural network, and gradient boosting machine (gbm) from the h2o package were explored and tested on the datasets.  

Finally, the "Regularized movie + user model" at lambda = 27.5 was determined to be the best algorithm that yielded the least RMSE of 0.8571358019 on the validation set.


## 2. Analysis  

## 2.1 Data wrangling  

The original edx and validation datasets were first generated containing 6 variables with 9,000,055 obs. and 999,999 obs., respectively.  This was the result of running the script provided by Harvardx. These datasets were then wrangled and cleaned by:

a.) extracting year_rated, month_rated, day_rated, weekday_rated, and wday_rated from original timestamp variable;  

b.) separating genres from original genres variable which were separated by pipe (|) to create new rows;  

c.) separating year_released from original title which is of the format "title (yyyy)"; and  

d.) removing specific observations for genres that indicated "(no genres listed)". There were only about 7 observations in edx and none in validation. 


```{r data_wrangling, echo=FALSE, message=FALSE, warning=FALSE}

### Data Wrangling

## Fix the dates from timestamp in both edx and validation datasets
# based of http://files.grouplens.org/datasets/movielens/ml-10m-README.html, the
# predictor timestamp represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970

library(lubridate)

# fix dates for edx dataset
edx <- edx %>% mutate(dates=as_datetime(timestamp,
                                        origin = lubridate::origin,
                                        tz = "UTC")) %>%
  mutate(year_rated = as.numeric(year(dates)),
         month_rated = as.numeric(month(dates)),
         day_rated = as.numeric(day(dates)),
         weekday_rated = weekdays(dates),
         wday_rated = as.numeric(wday(dates)))


# fix dates for validation set
validation <- validation %>% mutate(dates=as_datetime(timestamp,
                                                      origin = lubridate::origin,
                                                      tz = "UTC")) %>%
  mutate(year_rated = as.numeric(year(dates)),
         month_rated = as.numeric(month(dates)),
         day_rated = as.numeric(day(dates)),
         weekday_rated = weekdays(dates),
         wday_rated = as.numeric(wday(dates)))


# Split the genres in edx - split multi values in ine column into multiple rows
# separated by pipe |

# separate rows for edx based of genre
edx <- edx %>% separate_rows(genres, sep = "\\|")

# separate rows for validation based of genre
validation <- validation %>% separate_rows(genres, sep = "\\|")


## Extract year released in title which is of the format " (yyyy)" and convert:
###          userId as numeric
###          genres as factor
###          weekday as factor
###          title as character
# and then select the following variables:
# userId, movieId, title, rating, genres,
# year_rated, month_rated, day_rated, weekday_rated, wday_rated, year_released

# for edx
edx <- edx %>% mutate(title_for_trim = str_trim(title)) %>%
  extract(col=title_for_trim,
          into=c("title_real", "year_released"),
          regex="^(.*) \\(([0-9 \\-]*)\\)$",
          remove=F) %>%
  mutate(userId = as.numeric(userId),
         year_released=as.numeric(year_released),
         title_real=as.character(title_real),
         weekday_rated=as.factor(weekday_rated),
         genres=as.factor(genres)) %>%
  mutate(title=title_real) %>%
  select(userId, movieId, title, rating, genres,
         year_rated, month_rated, day_rated, weekday_rated, wday_rated, year_released)


# for validation
validation <- validation %>% mutate(title_for_trim = str_trim(title)) %>%
  extract(col=title_for_trim,
          into=c("title_real", "year_released"),
          regex="^(.*) \\(([0-9 \\-]*)\\)$",
          remove=F) %>%
  mutate(userId = as.numeric(userId),
         year_released=as.numeric(year_released),
         title_real=as.character(title_real),
         weekday_rated=as.factor(weekday_rated),
         genres=as.factor(genres)) %>%
  mutate(title=title_real) %>%
  select(userId, movieId, title, rating, genres,
         year_rated, month_rated, day_rated, weekday_rated, wday_rated, year_released)


## Check for and handle NAs in edx and validation, if any

## Explore unique values for vectors other than dates in dataset

# note that there were 7 obs of "no genres listed" in edx - these are equivalent to NAs
# since we have over 23 million obs in edx, there's no harm removing these 7 obs.
# as well there were 0 obs of "no genres listed" in validation

## Remove the records in edx and validation datasets where genres=="(no genres listed)"

edx <- edx %>% filter(!genres=="(no genres listed)") %>% droplevels()

validation <- validation %>% filter(!genres=="(no genres listed)") %>% droplevels()

```


After wrangling and cleaning, the edx and validation datasets grew to 11 variables containing 23,371,416 obs. and 2,595,763 obs., respectively.


## 2.2 Exploratory data analysis (EDA)

The EDA was performed on both the edx and validation datasets in terms of generating summary statistics, visualization, checking correlation, principal components, and variable importance:  
  

## 2.2.1 Generate numerical and character summary statistics

```{r stat_summary, echo=FALSE, message=FALSE, warning=FALSE}
library(xda)
print("edx statistical summary")
numSummary(edx)
print("edx character summary")
charSummary(edx)[, 1:4]

print("validation statistical summary")
numSummary(validation)
print("validation character summary")
charSummary(validation)[, 1:4]
```



## 2.2.2 Visualization

The following charts pertain to the distribution of rating vs. genres, year_released, year_rated, month_rated, day_rated, weekday_rated, movieId, and userId:


## 2.2.2.1 Distribution of edx ratings


```{r viz_01, echo=FALSE}
## Distribution of edx ratings 
# table(edx$rating)
#    0.5       1     1.5       2     2.5       3     3.5       4     4.5       5 
# 215932  844336  276711 1794242  874289 5467061 2110688 6730401 1418246 3639510

library(ggplot2)
edx %>%
  ggplot(aes(rating)) + 
  geom_histogram(bins = 20, color = "black") +
  xlab('Ratings') +
  coord_trans(y = "sqrt") +
  ggtitle("Distribution of edx Ratings")
```

The dependent variable, ratings, on the edx dataset does not follow a normal distribution.  


## 2.2.2.2 Distribution of edx ratings by genre

```{r viz_02, echo=FALSE}
edx %>% 
  group_by(rating, genres) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  ggplot(aes(x = factor(genres),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Genres') +
  coord_trans(y = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Distribution of edx Ratings by Genre")

```

Note that Drama, Comedy, Action, Thriller, and Adventure top the genres.



## 2.2.2.3 Distribution of edx ratings by year released

```{r viz_03, echo=FALSE}
edx %>% 
  group_by(rating, year_released) %>%
  summarize(n = n()) %>%
  arrange(desc(rating, n)) %>%
  ggplot(aes(x = factor(year_released),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Year Released') +
  coord_trans(y = "log10") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Distribution of edx Ratings by Year Released")

```

There were many ratings made for movies released between 1994 and 1999, with peak at 1995.



## 2.2.2.4 Distribution of edx ratings by year rated

```{r viz_04, echo=FALSE}
edx %>% 
  group_by(rating, year_rated) %>%
  summarize(n = n()) %>%
  arrange(desc(rating, n)) %>%
  ggplot(aes(x = factor(year_rated),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Year Rated') +
  coord_trans(y = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Distribution of edx Ratings by Year Rated")

```

The year 2000 was the year when there were many ratings made.



## 2.2.2.5 Distribution of edx ratings by month rated

```{r viz_05, echo=FALSE}
edx %>% 
  group_by(rating, month_rated) %>%
  summarize(n = n()) %>%
  arrange(desc(rating, n)) %>%
  ggplot(aes(x = factor(month_rated),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Month Rated') +
  coord_trans(y = "sqrt") +
  ggtitle("Distribution of edx Ratings by Month Rated")

```

It may be observed here that there were many ratings made between October to December, with peak at around November.



## 2.2.2.6 Distribution of edx ratings by day rated

```{r viz_06, echo=FALSE}
edx %>% 
  group_by(rating, day_rated) %>%
  summarize(n = n()) %>%
  arrange(desc(rating, n)) %>%
  ggplot(aes(x = factor(day_rated),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Day Rated') +
  coord_trans(y = "log10") +
  ggtitle("Distribution of edx Ratings by Day Rated")

```

It appears that the number of ratings peaked around the 20th day of the month.



## 2.2.2.7 Distribution of edx ratings by weekday rated

```{r viz_07, echo=FALSE}
edx %>% 
  group_by(rating, weekday_rated) %>%
  summarize(n = n()) %>%
  arrange(desc(rating, n)) %>%
  ggplot(aes(x = factor(weekday_rated,
                        c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Weekday Rated') +
  coord_trans(y = "sqrt") +
  ggtitle("Distribution of edx Ratings by Weekday Rated")

```

It appears that there are more ratings made around Tuesday of the week.  



## 2.2.2.8 Distribution of edx ratings of Top 1000 movieIds

```{r viz_08, echo=FALSE}

edx %>%
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Distribution of movieIds")

edx %>% 
  group_by(movieId, rating) %>%
  summarize(n = n()) %>%
  top_n(1000, rating) %>%
  arrange(desc(rating)) %>%
  ggplot(aes(x = factor(rating, c(seq(0.5, 5, 0.5))),
             y = n)) + 
  geom_boxplot() +
  geom_point() +
  xlab('Ratings of Top 1000 movieIds') +
  coord_trans(y = "log10") +
  ggtitle("Distribution of edx Top 1000 movieIds")

```

As depicted in the histogram, edx movieId somehow follows a normal distribution, and rating of 3 and 4 are prevalent in top 1000 movies.  



## 2.2.2.9 Distribution of edx ratings of Top 1000 userIds

```{r viz_09, echo=FALSE}
edx %>%
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() +
  ggtitle("Distribution of userIds")

```

As depicted in the histogram, edx userId is skewed to the right.  



## 2.2.2.10 Plot of movieId vs. userId for Top 1000 movieIds rated 5

```{r viz_10, echo=FALSE, message=FALSE, warning=FALSE}
edx %>% filter(rating==5) %>%
  top_n(1000, movieId) %>%
  group_by(movieId, userId, genres) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  ggplot(aes(x = userId,
             y = movieId,
             color=factor(genres))) + 
  geom_point() +
  coord_trans(x = "log10") +
  coord_trans(y = "log10") +
  xlab('userId') +
  ylab('movieId') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("edx movieId vs. userId for Top 1000 movieIds")

```


There seems to be no obvious trend here, but it appears that movieIds ranging from 1 to 61500 were rated 5 by most users.  




## 2.2.3 Correlation

This is to check the edx dataset if correlation exists between the independent variables: userId, movieId, genres, year_released, year_rated, month_rated, day_rated, and weekday_rated.  



```{r correlation_check, echo=FALSE}
## Check correlation
# remove title, and rating from edx, scale and convert remaining predictors to numeric
edx %>% select(-c(title,rating)) %>%
  mutate(genres = as.numeric(factor(genres)),
         weekday_rated = as.numeric(weekday_rated)) %>%
  as.matrix() %>%
  scale(center=TRUE, scale=TRUE) %>%
  cor()
```

It is apparent that a very slight positive correlation exists between movieId vs. year_rated and year_released at 0.374036 and 0.257266, respectively.  



## 2.2.4 Principal components analysis: edx

```{r PCA_check, echo=FALSE}
## Check principal components
edx %>% select(-c(title,rating)) %>%
  mutate(genres = as.numeric(factor(genres)),
         weekday_rated = as.numeric(weekday_rated)) %>%
  as.matrix() %>%
  scale(center=TRUE, scale=TRUE) %>%
  prcomp() %>%
  summary()
```
The first 7 components in edx dataset account for 84.702% of the variability.  



## 2.2.5 Variable importance

Using the earth package, the edx variables year_released, movieId, and year_rated were determined to be important.  (Note: running varimp may take a longer while esp. in ordinary 8 GB machines.)  


```{r varimp_earth, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Check variable importance
library(earth)
edx.scaled <- edx %>% select(userId, movieId, rating,
                             year_rated, month_rated, day_rated,
                             wday_rated, year_released) %>%
  scale(center=TRUE, scale=TRUE) %>%
  data.frame()

earth_edx <- earth(rating ~., data=edx.scaled)

plot(evimp(earth_edx, trim=FALSE))

earth_edx

```
  
```{r alternative_load_earth_edx, echo=FALSE, message=FALSE, warning=FALSE}
# alternative code to varimp above
# instead of running the code above, run this alternative code chunk
# load earth_edx that has been previously created by running the main R code
# and saved in local drive.
library(earth)

load("D:\\Harvardx_DataScience_ProfessionalCertificate\\Movielens_Capstone_Project_Release\\earth_edx.RDA")

evimp(earth_edx, trim=FALSE) %>% plot()

earth_edx
```  
  


## 2.3 Generate train_set and test_set from edx

The wrangled edx dataset contains 23,371,416 obs. of  11 variables. This was then split into 70%-30% proportions corresponding to train_set and test_set, respectively.

```{r train_test_split, echo=FALSE}
# Split edx into train_set and test_sets
# split edx into 70% train_set and 30% test_set

library(caret)
set.seed(1)
#set.seed(1, sample.kind="Rounding")

test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.30, list = FALSE)
train_set <- edx[-test_index,]

test_set <- edx[test_index,] %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

validation <- validation %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

print("train_set")
str(train_set)

print("test_set")
str(test_set)

print("validation")
str(validation)

```



## 3. Methods

With rating as the dependent variable, models will be built using the following independent variables:

a.) movieId  
b.) userId  
c.) genres  
d.) weekday_rated  
e.) year_rated  
f.) year_released  


These independent variables will be gradually added to the models starting with 'movieId', then 'movieId + userId', then 'movieId + userId + genres', ... and so on and so forth, on both the non-regularized and regularized models.  


The following eighteen models will be built and trained on the train_set with 16,359,990 obs. of 11 variables:

a.) Just the mean (naive) model  
b.) Non-regularized movie effect  
c.) Non-regularized movie + user effect  
d.) Non-regularized movie + user + genres effect  
e.) Non-regularized movie + user + genres + weekday_rated effect  
f.) Non-regularized movie + user + genres + weekday_rated + year_released effect  
g.) Non-regularized movie + user + genres + weekday_rated + year_released + year_rated effect  
h.) Regularized movie effect  
i.) Regularized movie + user effect  
j.) Regularized movie + user + genres effect  
k.) Regularized movie + user + genres + weekday_rated effect  
l.) Regularized movie + user + genres + weekday_rated + year_released effect  
m.) Regularized movie + user + genres + weekday_rated + year_released + year_rated effect  
n.) Linear regression (lm) method from stats package  
o.) Random forest model from h2o package  
p.) Generalized linear model (glm) from h2o package  
q.) Deep learning with (7,3) hidden neurons from h2o package  
r.) Gradient boosting machine (gbm) from h2o package  


Note that for simplicity, all the six independent variables will be applied outrightly, not gradually, on the last five models, which use the stats and h2o packages.  


The respective RMSEs of the models will then be calculated on both the test_set with 7,011,381 obs. of 11 variables and validation dataset with 2,595,763 obs.  However, only the RMSE of the validation set will be reported as the basis of determining the best model and grade.



```{r rmse_set_option, echo=FALSE}

options(digits=10)

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```


## 3.1 Just the average (naive)  

This is the simplest model and it assumes the same rating $\mu$ for all movies for all users under all circumstances. As discussed in Section 33.7.4 of the book (https://rafalab.github.io/dsbook/), the equation is:  
  
$$Y_{u,i} = {\mu} + \epsilon_{u,i}$$
where:  
$\mu$ = true rating for all movies  
$\epsilon_{u,i}$ = independent errors sampled from the same distribution centered at 0



```{r naive_average, echo=FALSE}
## just the average

# calculate mean rating of all movies in edx
mu_hat <- mean(train_set$rating)

# predict RMSE on test_set
justmean_rmse_test_set <- RMSE(test_set$rating, mu_hat)

# predict RMSE on validation_set
justmean_rmse_val_set <- RMSE(validation$rating, mu_hat)

results <- data.frame(method = "Just the mean",
                      #RMSE_test_set = justmean_rmse_test_set,
                      RMSE_validation = justmean_rmse_val_set)

results %>% knitr::kable()

```



The average from all ratings in the train_set $mu$ was calculated to be **`r mu_hat`** and this represents the predicted rating that any user will most likely provide for any movie. The RMSEs on the test_set and validation sets are **`r justmean_rmse_test_set`** and **`r justmean_rmse_val_set`**, respectively. The validation set's RMSE of  **`r justmean_rmse_val_set`** should be the maximum value, and anything above this should be worse.  



## 3.2 Non-regularized models  

## 3.2.1 Movie effect  

The naive model could be improved by adding the movie effect $b_i$.  As discussed in Section 33.7.5 of the book (https://rafalab.github.io/dsbook/), this value likewise is referred to as "bias", with the intuition that different movies are rated differently - meaning that certain movies are rated higher than others.  The formula that considers the effect of movie can be defined as:

$$Y_{u,i} = {\mu} + b_i + \epsilon_{u,i}$$
where: $b_i$ = bias for $movie_i$ and is just the average of ($Y_{u,i}$ - $\mu$) for each movie  


```{r nreg_movie, echo=FALSE}
## movie effect model
## Yu,i = μ + bi + ϵu,i

mu <- mean(train_set$rating)

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  mutate(rating_hat = mu + b_i) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  mutate(rating_hat = mu + b_i) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add rmse to results
results <-  results %>% add_row(method = "Movie effect",
                                #RMSE_test_set = movie_model_rmse_test_set,
                                RMSE_validation = movie_model_rmse_val_set)
# view results
results %>% knitr::kable()

```


The RMSE of adding movie effect on the test_set = **`r movie_model_rmse_test_set`**, while the RMSE on validation set is **`r movie_model_rmse_val_set`**, with the naive model likewise indicated for comparison.



## 3.2.2 Movie + user effect  

The effect of user is added to the movie effect as some users have the tendency of giving higher ratings to certain movies than other users.  Some users love every movie but some are even peeky, choosy, or hard-to-please.  Hence, there is considerable variability across users which could be represented by the user-specific effect or bias $b_u$. This simply means, as discussed in Section 33.7.6 of the book (https://rafalab.github.io/dsbook/), that if a cranky user (-$b_u$) gives a rating to a great movie (+$b_i$), the effects would counter each other so we could say that such user gave this great movie a 3 rather than a 5. 

The formula to add the user effect can be defined as:  

$$Y_{u,i} = {\mu} + b_i + b_u + \epsilon_{u,i}$$
where:  
$b_i$ = bias for $movie_i$  
$b_u$ = user-specific effect = average of ($Y_{u,i}$ - $\mu$ - $b_i$) for each user    


```{r nreg_movie_user, echo=FALSE}
## movie + user effect model
## Yu,i = μ + bi + bu + ϵu,i

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(rating_hat = mu + b_i + b_u) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_user_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(rating_hat = mu + b_i + b_u) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_user_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Movie + user effect",
                                #RMSE_test_set = movie_user_model_rmse_test_set,
                                RMSE_validation = movie_user_model_rmse_val_set)

results %>% knitr::kable()
```
  
  
The RMSEs on the test_set and validation datasets as a result of adding user effect are **`r movie_user_model_rmse_test_set`** and **`r movie_user_model_rmse_val_set`**, respectively.  RMSEs from previous models are likewise indicated for comparison.



## 3.2.3 Movie + user + genres effect  

The effect of genres is added to the movie and user effects as certain users may be biased to give high ratings to movies of specific genres than other users.  For instance, some users may give higher ratings to thriller movies than documentary. This is evident during the exploratory data analysis in Section 2.2.2.2 of this material in that drama, comedy, action, and thriller movies were rated higher than for example, documentary or film-noir.  The formula to add the genre effect can be defined as: 
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + \epsilon_{u,i}$$

where:  
$b_i$ = bias for $movie_i$  
$b_u$ = user-specific effect  
$b_g$ = genre-specific effect = average of ($Y_{u,i}$ - $\mu$ - $b_i$ - $b_u$) for each genre



```{r nreg_movie_user_genres, echo=FALSE}
## movie + user + genre effect model
## Yu,i = μ + bi + bu + bg + ϵu,i

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

genres_avgs <- train_set %>%
  mutate(genres=factor(genres)) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_user_genres_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_user_genres_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Movie + user + genres effect",
                                #RMSE_test_set = movie_user_genres_model_rmse_test_set,
                                RMSE_validation = movie_user_genres_model_rmse_val_set)

results %>% knitr::kable()

```



The RMSEs on the test_set and validation datasets as a result of adding genre effect are **`r movie_user_genres_model_rmse_test_set`** and **`r movie_user_genres_model_rmse_val_set`**, respectively.



## 3.2.4 Movie + user + genres + weekday_rated effect

Based of the exploratory data analysis in Section 2.2.2.7 of this material, the count of ratings were comparatively higher around Monday or Tuesday of the week.  There seems to be no clear explanation for this but perhaps users who watched movies during the weekend may have reflected the ratings on either Monday or Tuesday the following week as they were prompted and/or were available to rate.  The formula to add the weekday effect could be defined as:  
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + \epsilon_{u,i}$$
where:  
$b_i$ = bias for $movie_i$  
$b_u$ = user effect  
$b_g$ = genre effect  
$b_d$ = weekday effect = average of ($Y_{u,i}$ - $\mu$ - $b_i$ - $b_u$ - $b_g$) for each weekday



```{r nreg_movie_user_genres_weekday, echo=FALSE}
## movie + user + genre + weekday_rated bias model
## Yu,i = μ + bi + bu + bg + bd + ϵu,i

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

genres_avgs <- train_set %>%
  mutate(genres=factor(genres)) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

weekday_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  group_by(weekday_rated) %>%
  summarize(b_d = mean(rating - mu - b_i - b_u - b_g))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_user_genres_day_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_user_genres_day_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Movie + user + genres + weekday_rated effect",
                                #RMSE_test_set = movie_user_genres_day_model_rmse_test_set,
                                RMSE_validation = movie_user_genres_day_model_rmse_val_set)

results %>% knitr::kable()

```


The RMSEs on the test_set and validation datasets as a result of adding weekday effect are **`r movie_user_genres_day_model_rmse_test_set`** and **`r movie_user_genres_day_model_rmse_val_set`**, respectively.



## 3.2.5 Movie + user + genres + weekday_rated + year_released effect  

The year_released variable was one of the important variables identified after running the earth package in the main R code and as indicated in Section 2.2.5 Variable importance of this material. As well, the distribution of edx ratings by year_released in Section 2.2.2.3 indicate that there were many ratings made sometime in mid 1990's, specifically 1994.  Perhaps, there were many great movies that were released during this year.  The formula to add the year_released effect could be written as:  
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + y_r + \epsilon_{u,i}$$
where:  
$b_i$ = bias for $movie_i$  
$b_u$ = user effect  
$b_g$ = genre effect  
$b_d$ = weekday  effect  
$y_r$ = year released effect = average of ($Y_{u,i}$ - $\mu$ - $b_i$ - $b_u$ - $b_g$ - $b_d$) for each year_released


The calculated RMSE on the validation set is shown below:

```{r nreg_movie_user_genres_weekday_yearrel, echo=FALSE}
## movie + user + genre + weekday_rated + year_released  bias model
## Yu,i = μ + bi + bu + bg + bd + yr + ϵu,i

mu <- mean(train_set$rating)

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

genres_avgs <- train_set %>%
  mutate(genres=factor(genres)) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

weekday_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  group_by(weekday_rated) %>%
  summarize(b_d = mean(rating - mu - b_i - b_u - b_g))

# year_released
yearreleased_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  group_by(year_released) %>%
  summarize(y_r = mean(rating - mu - b_i - b_u - b_g - b_d))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  left_join(yearreleased_avgs, by='year_released') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_user_genres_day_yr_released_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  left_join(yearreleased_avgs, by='year_released') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_user_genres_day_yr_released_model_rmse_val_set <-
  RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Movie + user + genres + weekday_rated + year_released effect",
                                #RMSE_test_set = movie_user_genres_day_yr_released_model_rmse_test_set,
                                RMSE_validation = movie_user_genres_day_yr_released_model_rmse_val_set)

results %>% knitr::kable()

```


The RMSEs on the test_set and validation datasets as a result of adding year released effect are **`r movie_user_genres_day_yr_released_model_rmse_test_set`** and **`r movie_user_genres_day_yr_released_model_rmse_val_set`**, respectively.  



## 3.2.6 Movie + user + genres + weekday_rated + year_released + year_rated effect  

In addition to year_released, the year_rated variable was likewise one of the important variables determined after running the earth package in the main R code.  This was discussed in indicated in Section 2.2.5 Variable importance of this material.  The distribution of edx ratings by year_rated in Section 2.2.2.4 indicate that there were many ratings made sometime in during the years 1996 and 2000.  The formula to add the year_rated effect could be written as:

$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + y_r + y_a + \epsilon_{u,i}$$
where:  
$b_i$ = bias for $movie_i$  
$b_u$ = user effect  
$b_g$ = genre effect  
$b_d$ = weekday  effect  
$y_r$ = year released effect  
$y_a$ = year rated effect = average of ($Y_{u,i}$ - $\mu$ - $b_i$ - $b_u$ - $b_g$ - $b_d$ - $y_r$) for each year rated


```{r nreg_movie_user_genres_weekday_yearrel_yearra, echo=FALSE}
## movie + user + genre + weekday_rated + year_released + year_rated  bias model
## Yu,i = μ + bi + bu + bg + bd + yr + ya + ϵu,i

mu <- mean(train_set$rating)

movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

genres_avgs <- train_set %>%
  mutate(genres=factor(genres)) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

weekday_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  group_by(weekday_rated) %>%
  summarize(b_d = mean(rating - mu - b_i - b_u - b_g))

# year_released
yearreleased_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  group_by(year_released) %>%
  summarize(y_r = mean(rating - mu - b_i - b_u - b_g - b_d))

# year_rated
yearrated_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  left_join(yearreleased_avgs, by='year_released') %>%
  group_by(year_rated) %>%
  summarize(y_a = mean(rating - mu - b_i - b_u - b_g - b_d - y_r))

# predict the ratings on the test_set
predicted_ratings_test <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  left_join(yearreleased_avgs, by='year_released') %>%
  left_join(yearrated_avgs, by='year_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r + y_a) %>%
  pull(rating_hat)

# predict RMSE on test_set
movie_user_genres_day_yr_rated_released_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation set
predicted_ratings_val_set <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  left_join(weekday_avgs, by='weekday_rated') %>%
  left_join(yearreleased_avgs, by='year_released') %>%
  left_join(yearrated_avgs, by='year_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r + y_a) %>%
  pull(rating_hat)

# predict RMSE on validation
movie_user_genres_day_yr_rated_released_model_rmse_val_set <-
  RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>%
  add_row(method = "Movie + user + genres + weekday_rated + year_released + year_rated effect",
          #RMSE_test_set = movie_user_genres_day_yr_rated_released_model_rmse_test_set,
          RMSE_validation = movie_user_genres_day_yr_rated_released_model_rmse_val_set)

results %>% knitr::kable()

```


The RMSEs on the test_set and validation datasets as a result of adding year rated effect are **`r movie_user_genres_day_yr_rated_released_model_rmse_test_set`** and **`r movie_user_genres_day_yr_rated_released_model_rmse_val_set`**, respectively.



## 3.3 Regularized models  

The precision of the movie bias $b_i$ is dependent on the number of occurrences (samples) that such movies were rated - i.e., the more ratings, the more precise $b_i$ would be.  However, while there are certain movies that were rated many times, there are likewise certain movies that were rated only once or a few times. Hence, there is a need to put more weight on the movies that have many ratings, and lesser weight on those that have less. 

As discussed in Section 33.9 of the book (https://rafalab.github.io/dsbook/), regularization penalizes large estimates that are formed using small sample sizes.  This is solved by introducing the penalty parameter $\lambda$, and the idea is to constrain the total variability of the effect sizes.  Hence, in the formula below:

$${b_{i}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu})$$


a.) If the number of ratings made for $movie_i$ is very large, then the penalty $\lambda$ is effectively ignored because the term ($\lambda$ + $n_{i}$) will approximate $n_{i}$. In this case we have a stable estimate of $b_{i}(\lambda)$.

b.) On the contrary, if the number of ratings made for $movie_i$ is very small, then the penalty $\lambda$ becomes large because the term ($\lambda$ + $n_{i}$) will approximate $\lambda$. In this case, the estimate $b_{i}(\lambda)$ is shrunk to 0; thus the larger the $\lambda$, the more we shrink.  

In the succeeding sections for regularized models, the penalty $\lambda$ will become the tuning parameter to reduce the RMSEs. It will first be calculated from the train_set and then the $\lambda$ with the least RMSE will be introduced into the models to predict the ratings and estimate the RMSE on both the test_set and validation datasets. And aside from the formulas and RMSE tables, graphs of lambdas vs. RMSEs will be presented along with with the values of the best-tuned lambda for each of the models.


## 3.3.1 Movie effect  
  
The formula used to regularize movie effect $b_i$ is: 

$$Y_{u,i} = {\mu} + b_i + \epsilon_{u,i}$$
$$where:
{b_{i}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu})$$
  
```{r reg_movie_graph, echo=FALSE}  
# define a sequence of lambdas
lambdas <- seq(0, 50, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    mutate(rating_hat = mu + b_i) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(test_set$rating, predicted_ratings_test))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) + ggtitle("Regularized Movie: Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie <- lambdas[which.min(rmses)]
# 1.75
```



```{r reg_movie, echo=FALSE}
# predict the ratings on the test_set at λ = 1.75
lambda <- 1.75

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  mutate(rating_hat = mu + b_i) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)

# predict the ratings on validation at λ = 1.75
lambda <- 1.75

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  mutate(rating_hat = mu + b_i) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie model",
                                #RMSE_test_set = reg_movie_model_rmse_test_set,
                                RMSE_validation = reg_movie_model_rmse_val_set)

results %>% knitr::kable()

```


The best-tuned $\lambda$ with the least RMSE to regularize movie effect $b_i$ = **`r lambda_movie`**. The RMSEs on the test_set and validation datasets as a result of regularizing movie effect $b_i$ are **`r reg_movie_model_rmse_test_set`** and **`r reg_movie_model_rmse_val_set`**, respectively.



## 3.3.2 Movie + user effect

The formula used to regularize user effect $b_u$ is: 

$$Y_{u,i} = {\mu} + b_i + b_u + \epsilon_{u,i}$$
$$where:
{b_{u}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu} - b_i)$$


```{r reg_movie_user_graph, echo=FALSE}
# define a sequence of lambdas
lambdas <- seq(0, 50, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized user b_u
  b_u <- train_set %>%
    group_by(userId) %>%
    summarize(b_u = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    mutate(rating_hat = mu + b_i + b_u) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(predicted_ratings_test, test_set$rating))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) + ggtitle("Regularized Movie + User: Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie_user <- lambdas[which.min(rmses)]
# 27.5
```



```{r reg_movie_user, echo=FALSE}
# predict the ratings on the test_set at λ = 27.5
lambda <- 27.5

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- test_set %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  mutate(rating_hat = mu + b_i + b_u) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_user_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation at λ = 27.5
lambda <- 27.5

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- validation %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  mutate(rating_hat = mu + b_i + b_u) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_user_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie + user model",
                                #RMSE_test_set = reg_movie_user_model_rmse_test_set,
                                RMSE_validation = reg_movie_user_model_rmse_val_set)

results %>% knitr::kable()

```


The best-tuned $\lambda$ with the least RMSE to regularize user effect $b_u$ = **`r lambda_movie_user`**. The RMSEs on the test_set and validation datasets as a result of regularizing user effect $b_u$ are **`r reg_movie_user_model_rmse_test_set`** and **`r reg_movie_user_model_rmse_val_set`**, respectively.



## 3.3.3 Movie + user + genres effect  
  
The formula used to regularize genre effect $b_g$ is: 

$$Y_{u,i} = {\mu} + b_i + b_u + b_g + \epsilon_{u,i}$$
$$where:
{b_{g}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu} - b_i - b_u)$$


```{r reg_movie_user_genre_graph, echo=FALSE}
# define a sequence of lambdas
lambdas <- seq(0, 50, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized user b_u
  b_u <- train_set %>%
    group_by(userId) %>%
    summarize(b_u = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized genres b_g
  b_g <- train_set %>%
    group_by(genres) %>%
    summarize(b_g = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    mutate(rating_hat = mu + b_i + b_u + b_g) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(predicted_ratings_test, test_set$rating))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) + ggtitle("Regularized Movie + User + Genres: Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie_user_genre <- lambdas[which.min(rmses)]
# 33.25
```



```{r movie_user_genres, echo=FALSE}
# predict the ratings on the test_set at λ = 33.25
lambda <- 33.25

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- test_set %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- test_set %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_user_genres_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation at λ = 33.25
lambda <- 33.25

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- validation %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- validation %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_user_genres_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie + user + genres model",
                                #RMSE_test_set = reg_movie_user_genres_model_rmse_test_set,
                                RMSE_validation = reg_movie_user_genres_model_rmse_val_set)

results %>% knitr::kable()

```


The best-tuned $\lambda$ with the least RMSE to regularize genre effect $b_g$ = **`r lambda_movie_user_genre`**. The RMSEs on the test_set and validation datasets as a result of regularizing genre effect $b_g$ are **`r reg_movie_user_genres_model_rmse_test_set`** and **`r reg_movie_user_genres_model_rmse_val_set`**, respectively.



## 3.3.4 Movie + user + genres + weekday_rated effect  
  
The formula used to regularize weekday effect $b_d$ is:  
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + \epsilon_{u,i}$$
$$where:
{b_{d}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu} - b_i - b_u - b_g)$$

  
```{r reg_movie_user_genre_weekday_graph, echo=FALSE}
# define a sequence of lambdas
lambdas <- seq(0, 70, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized user b_u
  b_u <- train_set %>%
    group_by(userId) %>%
    summarize(b_u = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized genres b_g
  b_g <- train_set %>%
    group_by(genres) %>%
    summarize(b_g = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized weekday_rated b_d
  b_d <- train_set %>%
    group_by(weekday_rated) %>%
    summarize(b_d = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    left_join(b_d, by='weekday_rated') %>%
    mutate(rating_hat = mu + b_i + b_u + b_g + b_d) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(predicted_ratings_test, test_set$rating))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) +
  ggtitle("Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie_user_genre_weekday <- lambdas[which.min(rmses)]
# 33.25
```



```{r movie_user_genres_weekday, echo=FALSE}
# predict the ratings on the test_set at λ = 33.25
lambda <- 33.25

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- test_set %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- test_set %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- test_set %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_user_genres_day_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation at λ = 33.25
lambda <- 33.25

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- validation %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- validation %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- validation %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_user_genres_day_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie + user + genres + weekday_rated model",
                                #RMSE_test_set = reg_movie_user_genres_day_model_rmse_test_set,
                                RMSE_validation = reg_movie_user_genres_day_model_rmse_val_set)

results %>% knitr::kable()

```


The best-tuned $\lambda$ with the least RMSE to regularize weekday effect $b_d$ = **`r lambda_movie_user_genre_weekday`**. The RMSEs on the test_set and validation datasets as a result of regularizing weekday effect $b_d$ are **`r reg_movie_user_genres_day_model_rmse_test_set`** and **`r reg_movie_user_genres_day_model_rmse_val_set`**, respectively.



## 3.3.5 Movie + user + genres + weekday_rated + year_released effect  
  
The formula used to regularize year released effect $y_r$ is:  
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + y_r + \epsilon_{u,i}$$
$$where:{y_{r}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu} - b_i - b_u - b_g - b_d)$$  
  
```{r reg_movie_user_genre_weekday_yr_released_graph, echo=FALSE}
# define a sequence of lambdas
lambdas <- seq(0, 100, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized user b_u
  b_u <- train_set %>%
    group_by(userId) %>%
    summarize(b_u = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized genres b_g
  b_g <- train_set %>%
    group_by(genres) %>%
    summarize(b_g = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized weekday_rated b_d
  b_d <- train_set %>%
    group_by(weekday_rated) %>%
    summarize(b_d = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized year_released y_r
  y_r <- train_set %>%
    group_by(year_released) %>%
    summarize(y_r = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    left_join(b_d, by='weekday_rated') %>%
    left_join(y_r, by='year_released') %>%
    mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(predicted_ratings_test, test_set$rating))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) +
  ggtitle("Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie_user_genre_weekday_yr_released <- lambdas[which.min(rmses)]
# 42.75
```



```{r movie_user_genres_weekday_yearreleased, echo=FALSE}
# predict the ratings on the test_set at λ = 42.75
lambda <- 42.75
# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- test_set %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- test_set %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- test_set %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_released y_r
y_r_reg <- test_set %>%
  group_by(year_released) %>%
  summarize(y_r = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  left_join(y_r_reg, by='year_released') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_user_genres_day_yr_released_model_rmse_test_set <-
  RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation at λ = 42.75
lambda <- 42.75

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- validation %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- validation %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- validation %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_released y_r
y_r_reg <- validation %>%
  group_by(year_released) %>%
  summarize(y_r = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  left_join(y_r_reg, by='year_released') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_user_genres_day_yr_released_model_rmse_val_set <-
  RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie + user + genres + weekday_rated + year_released model",
                                #RMSE_test_set = reg_movie_user_genres_day_yr_released_model_rmse_test_set,
                                RMSE_validation = reg_movie_user_genres_day_yr_released_model_rmse_val_set)

results %>% knitr::kable()

```



The best-tuned $\lambda$ with the least RMSE to regularize year released effect $y_r$ = **`r lambda_movie_user_genre_weekday_yr_released`**. The RMSEs on the test_set and validation datasets as a result of regularizing year released effect $y_r$ are **`r reg_movie_user_genres_day_yr_released_model_rmse_test_set`** and **`r reg_movie_user_genres_day_yr_released_model_rmse_val_set`**, respectively.



## 3.3.6 Movie + user + genres + weekday_rated + year_released + year_rated effect  
  
The formula used to regularize year rated effect $y_a$ is: 
  
$$Y_{u,i} = {\mu} + b_i + b_u + b_g + b_d + y_r + y_a + \epsilon_{u,i}$$
$$ where:
{y_{a}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - {\mu} - b_i - b_u - b_g - b_d - y_r)$$
  
```{r reg_movie_user_genre_weekday_yr_released_rated_graph, echo=FALSE}
# define a sequence of lambdas
lambdas <- seq(0, 100, 0.25)

# predict rating and RMSE on test_set and determine optimal lambda value
rmses <- sapply(lambdas, function(lambda){
  
  # calculate mu
  mu <- mean(train_set$rating)
  
  # calculate regularized movie b_i
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized user b_u
  b_u <- train_set %>%
    group_by(userId) %>%
    summarize(b_u = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized genres b_g
  b_g <- train_set %>%
    group_by(genres) %>%
    summarize(b_g = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized weekday_rated b_d
  b_d <- train_set %>%
    group_by(weekday_rated) %>%
    summarize(b_d = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized year_released y_r
  y_r <- train_set %>%
    group_by(year_released) %>%
    summarize(y_r = (1/(lambda + n())) * sum(rating - mu))
  
  # calculate regularized year_rated y_a
  y_a <- train_set %>%
    group_by(year_rated) %>%
    summarize(y_a = (1/(lambda + n())) * sum(rating - mu))
  
  # predict the ratings on test set
  predicted_ratings_test <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    left_join(b_d, by='weekday_rated') %>%
    left_join(y_r, by='year_released') %>%
    left_join(y_a, by='year_rated') %>%
    mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r + y_a) %>%
    pull(rating_hat)
  
  # compute the RMSE
  return(RMSE(predicted_ratings_test, test_set$rating))
})

# plot the RMSEs and lambdas
qplot(lambdas, rmses) +
  ggtitle("Plot of lambdas vs. RMSEs")

# determine which lambda with least rmse
lambda_movie_user_genre_weekday_yr_released_rated <- lambdas[which.min(rmses)]
# 50.0
```


```{r movie_user_genres_weekday_year_released_rated, echo=FALSE}
# predict the ratings on the test_set at λ = 50.0
lambda <- 50.0
# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- test_set %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- test_set %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- test_set %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- test_set %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_released y_r
y_r_reg <- test_set %>%
  group_by(year_released) %>%
  summarize(y_r = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_rated y_a
y_a_reg <- test_set %>%
  group_by(year_rated) %>%
  summarize(y_a = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on test set
predicted_ratings_test <- test_set %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  left_join(y_r_reg, by='year_released') %>%
  left_join(y_a_reg, by='year_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r + y_a) %>%
  pull(rating_hat)

# predict RMSE on test_set
reg_movie_user_genres_day_yr_rated_released_model_rmse_test_set <-
  RMSE(test_set$rating, predicted_ratings_test)


# predict the ratings on validation at λ = 50.0
lambda <- 50.0

# calculate mu
mu <- mean(train_set$rating)

# calculate regularized movie b_i
b_i_reg <- validation %>%
  group_by(movieId) %>%
  summarize(b_i = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized user b_u
b_u_reg <- validation %>%
  group_by(userId) %>%
  summarize(b_u = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized genres b_g
b_g_reg <- validation %>%
  group_by(genres) %>%
  summarize(b_g = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized weekday_rated b_d
b_d_reg <- validation %>%
  group_by(weekday_rated) %>%
  summarize(b_d = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_released y_r
y_r_reg <- validation %>%
  group_by(year_released) %>%
  summarize(y_r = (1/(lambda + n())) * sum(rating - mu))

# calculate regularized year_rated y_a
y_a_reg <- validation %>%
  group_by(year_rated) %>%
  summarize(y_a = (1/(lambda + n())) * sum(rating - mu))

# predict the ratings on validation
predicted_ratings_val_set <- validation %>%
  left_join(b_i_reg, by='movieId') %>%
  left_join(b_u_reg, by='userId') %>%
  left_join(b_g_reg, by='genres') %>%
  left_join(b_d_reg, by='weekday_rated') %>%
  left_join(y_r_reg, by='year_released') %>%
  left_join(y_a_reg, by='year_rated') %>%
  mutate(rating_hat = mu + b_i + b_u + b_g + b_d + y_r + y_a) %>%
  pull(rating_hat)

# predict RMSE on validation
reg_movie_user_genres_day_yr_rated_released_model_rmse_val_set <-
  RMSE(validation$rating, predicted_ratings_val_set)

# add the results
results <-  results %>% add_row(method = "Regularized movie + user + genres + weekday_rated + year_released + year_rated",
                                #RMSE_test_set = reg_movie_user_genres_day_yr_rated_released_model_rmse_test_set,
                                RMSE_validation = reg_movie_user_genres_day_yr_rated_released_model_rmse_val_set)

results %>% knitr::kable()
```


The best-tuned $\lambda$ with the least RMSE to regularize year rated effect $y_a$ = **`r lambda_movie_user_genre_weekday_yr_released_rated`**. The RMSEs on the test_set and validation datasets as a result of regularizing year rated effect $y_a$ are **`r reg_movie_user_genres_day_yr_rated_released_model_rmse_test_set`** and **`r reg_movie_user_genres_day_yr_rated_released_model_rmse_val_set`**, respectively.




## 3.4 linear regression (lm) method

The formula to implement linear model with rating $Y_{u,i}$ as the dependent variable and userId, movieId, genres, weekday_rated, year_released, and year_rated as independent variables is given by:  
  
$$Y_{u,i} = b_0 + b_1movieId + b_2userId + b_3genres + b_4weekday_{rated} + b_5year_{released} + b_6year_{rated}$$
where:  
$b_0$ = y-intercept  
$b_1$ = coefficient for movie effect  
$b_2$ = coefficient for user effect  
$b_3$ = coefficient for genre effect  
$b_4$ = coefficient for weekday  effect  
$b_5$ = coefficient for year released effect  
$b_6$ = coefficient for year rated effect  
  
  

```{r stats_lm, echo=FALSE, message=FALSE, warning=FALSE}

# train the stats lm model
# dependent var y = rating
# independent vars = userId, movieId, genres, year_rated, weekday_rated, year_released
# Yu,i = rating = userId + movieId + genres + weekday_rated + year_released + year_rated
library(stats)

regression_stats_lm <- lm(rating ~ userId + movieId + genres + year_rated + weekday_rated + year_released,
                          data=train_set)



# predict the ratings on the test_set
predicted_ratings_test <- predict(regression_stats_lm, test_set)

# predict RMSE on test_set
stats_lm_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test)
# 1.037941151

# predict the ratings on validation set
predicted_ratings_val_set <- predict(regression_stats_lm, validation)

# predict RMSE on validation
stats_lm_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set)
# 1.038492217

# add the results
results <-  results %>% add_row(method = "stats linear regression (lm) method",
                                #RMSE_test_set = stats_lm_model_rmse_test_set,
                                RMSE_validation = stats_lm_model_rmse_val_set)


results %>% knitr::kable()

```
  
  
The RMSEs on the test_set and validation datasets as a result of running the linear model from the stats package based of the above formula are **`r stats_lm_model_rmse_test_set`** and **`r stats_lm_model_rmse_val_set`**, respectively.
  
  
  
  
  
  
########  
  
  
  
  
  
  
  
**Brief Introduction to h2o Library:**
  
  
The remaining four sections attempt to predict the rating $Y_{u,i}$ using the h2o library.
  
The h2o library is a scalable open-source machine learning library that features AutoML.  This article from R-bloggers (https://www.r-bloggers.com/5-reasons-to-learn-h2o-for-high-performance-machine-learning/) caught my attention, and this is the reason h2o models are included in this project. According to the author, there are 5 reasons for using h2o:

a.) h2o AutoML automates the machine learning workflow, which includes automatic training and tuning of many models.
    
b.) Scalable on Local Compute: distributed, in-memory processing speeds up computations
    
c.) Spark integration & GPU support: the result is 100x faster training than traditional ML.
    
d.) Superior performance: best algorithms, optimized and ensembled: The most popular algorithms are incorporated including GLM, random forest, GBM and more.
    
e.) Production ready, e.g. docker containers
  
  
  
Similar to lm() model, the formula that will be used in modeling using the h2o library is:  

$$Y_{u,i} = movieId + userId + genres + weekday_{rated} + year_{released} + year_{rated} + \epsilon$$
  
  
  
Finally, the hyper-parameters from the following h2o models below have not yet been fine-tuned due to time constraints in learning how to use the library and so determining the best-tuned parameters is beyond the scope of this project.  As well, there is no guarantee that the models created here using h2o will significantly reduce the RMSE on the movielens dataset.  But then out of curiousity, it may be worth exploring and trying it out in this dataset and project in particular, and for other projects in general. Nevertheless for more information and other details, h2o tutorials are available in http://docs.h2o.ai/h2o-tutorials/latest-stable/.
  
  
  
  
  
## 3.5 h2o random forest implementation
  

The h2o library need to first be loaded and initialized.  As well, the train_set, test_set, and validation datasets should be converted to h2o instance using the folowing code:
  
  
  
```{r train_test_convert_to_h2o, echo=TRUE, message=FALSE, warning=FALSE}
library(h2o)
h2o.init()
train.h2o <- as.h2o(train_set)
test.h2o <- as.h2o(test_set)
validation.h2o <- as.h2o(validation)

```

  

```{r h2o_randomforest, message=FALSE, warning=FALSE, include=FALSE}

# train the h2o randomforest model
# Yu,i = rating = userId + movieId + genres + weekday_rated + year_released + year_rated
regression_rf_h2o <- h2o.randomForest(x = -c(3,7,8,10),
                                      y = 4,
                                      training_frame = train.h2o,
                                      seed=1,
                                      nfolds = 10,
                                      ntrees = 1000,
                                      mtries = 3,
                                      max_depth = 4)

# h2o.performance(regression_rf_h2o)

# predict the ratings on the test_set
predicted_ratings_test <- as.data.frame(h2o.predict(regression_rf_h2o, test.h2o))

# predict RMSE on test_set
rf_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test$predict)


# predict the ratings on validation set
predicted_ratings_val_set <- as.data.frame(h2o.predict(regression_rf_h2o, validation.h2o))

# predict RMSE on validation
rf_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set$predict)

# add the results
results <-  results %>% add_row(method = "h2o random forest model",
                                #RMSE_test_set = rf_model_rmse_test_set,
                                RMSE_validation = rf_model_rmse_val_set)

```
  
  

```{r h2o_rf_results, echo=FALSE, message=FALSE, warning=FALSE}
results %>% knitr::kable()

```
  
  
  
The RMSEs on the test_set and validation datasets as a result of running the h2o random forests model are **`r rf_model_rmse_test_set `** and **`r rf_model_rmse_val_set`**, respectively.
  
  
  
  
## 3.6 h2o generalized linear model (glm) implementation
  
  
  
```{r h2o_glm, message=FALSE, warning=FALSE, include=FALSE}

# train the h2o glm model
# dependent var y = rating
# independent vars = userId, movieId, genres, year_rated, weekday_rated, year_released
# Yu,i = rating = userId + movieId + genres + weekday_rated + year_released + year_rated
regression_glm_h2o <- h2o.glm(x = -c(3,7,8,10),
                              y = 4,
                              training_frame = train.h2o,
                              seed=1,
                              nfolds = 10,
                              family="gaussian")

#h2o.performance(regression_glm_h2o)

# predict the ratings on the test_set
predicted_ratings_test <- as.data.frame(h2o.predict(regression_glm_h2o, test.h2o))

# predict RMSE on test_set
glm_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test$predict)


# predict the ratings on validation set
predicted_ratings_val_set <- as.data.frame(h2o.predict(regression_glm_h2o, validation.h2o))

# predict RMSE on validation
glm_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set$predict)

# add the results
results <-  results %>% add_row(method = "h2o glm model",
                                #RMSE_test_set = glm_model_rmse_test_set,
                                RMSE_validation = glm_model_rmse_val_set)

```

```{r h2o_glm_results, echo=FALSE, message=FALSE, warning=FALSE}
results %>% knitr::kable()

```
  
  
The RMSEs on the test_set and validation datasets as a result of running the h2o generalized linear model (glm) are **`r glm_model_rmse_test_set`** and **`r glm_model_rmse_val_set`**, respectively.
  
  
  
  
## 3.7 h2o deep neural network implementation  
  
  
```{r h2o_deeplearning, message=FALSE, warning=FALSE, include=FALSE}

# train the h2o deeplearning model
# dependent var y = rating
# independent vars = userId, movieId, genres, year_rated, weekday_rated, year_released
# Yu,i = rating = userId + movieId + genres + weekday_rated + year_released + year_rated
regression_deeplearn_h2o <- h2o.deeplearning(x = -c(3,7,8,10),
                                             y = 4,
                                             training_frame = train.h2o,
                                             seed=1,
                                             nfolds = 10,
                                             epochs=10,
                                             hidden = c(7,3),
                                             activation="Rectifier",
                                             overwrite_with_best_model=TRUE,
                                             use_all_factor_levels = TRUE,
                                             variable_importances = TRUE,
                                             export_weights_and_biases = TRUE,
                                             verbose=TRUE)

#h2o.performance(regression_deeplearn_h2o)

# predict the ratings on the test_set
predicted_ratings_test <- as.data.frame(h2o.predict(regression_deeplearn_h2o, test.h2o))

# predict RMSE on test_set
deeplearn_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test$predict)


# predict the ratings on validation set
predicted_ratings_val_set <- as.data.frame(h2o.predict(regression_deeplearn_h2o, validation.h2o))

# predict RMSE on validation
deeplearn_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set$predict)

# add the results
results <-  results %>% add_row(method = "h2o deep learning: (7,3) hidden layers",
                                #RMSE_test_set = deeplearn_model_rmse_test_set,
                                RMSE_validation = deeplearn_model_rmse_val_set)

```

```{r h2o_deeplearn_results, echo=FALSE, message=FALSE, warning=FALSE}
results %>% knitr::kable()

```
  
  
The RMSEs on the test_set and validation datasets as a result of running the h2o deep learning model, with (7,3) hidden layers, and using rectifier activation function are **`r deeplearn_model_rmse_test_set`** and **`r deeplearn_model_rmse_val_set`**, respectively.
  
  
  
  
  
## 3.8 h2o gradient boosting machine (gbm) implementation
  
  
```{r h2o_gbm, message=FALSE, warning=FALSE, include=FALSE}

# train the h2o gbm model
# dependent var y = rating
# independent vars = userId, movieId, genres, year_rated, weekday_rated, year_released
# Yu,i = rating = userId + movieId + genres + weekday_rated + year_released + year_rated
regression_gbm_h2o <- h2o.gbm(x = -c(3,7,8,10),
                              y = 4,
                              training_frame = train.h2o,
                              seed=1,
                              nfolds = 10,
                              distribution="gaussian",
                              stopping_metric = "RMSE",
                              categorical_encoding = "AUTO",
                              verbose=TRUE)

#h2o.performance(regression_gbm_h2o)

# predict the ratings on the test_set
predicted_ratings_test <- as.data.frame(h2o.predict(regression_gbm_h2o, test.h2o))

# predict RMSE on test_set
gbm_model_rmse_test_set <- RMSE(test_set$rating, predicted_ratings_test$predict)


# predict the ratings on validation set
predicted_ratings_val_set <- as.data.frame(h2o.predict(regression_gbm_h2o, validation.h2o))

# predict RMSE on validation
gbm_model_rmse_val_set <- RMSE(validation$rating, predicted_ratings_val_set$predict)

# add the results
results <-  results %>% add_row(method = "h2o gradient boosting machine (gbm)",
                                #RMSE_test_set = gbm_model_rmse_test_set,
                                RMSE_validation = gbm_model_rmse_val_set)

```
  
  
```{r h2o_gbm_results, echo=FALSE, message=FALSE, warning=FALSE}
results %>% knitr::kable()

```
  
  
The RMSEs on the test_set and validation datasets as a result of running the h2o gradient boosting machine (gbm) model are **`r gbm_model_rmse_test_set`** and **`r gbm_model_rmse_val_set`**, respectively.
  
  
  
Finally, the h2o library needs to be shutdown using the code below:
  
  
  
```{r h2o_shutdown, echo=TRUE, message=FALSE, warning=FALSE}
h2o.shutdown()

```
  
  
  
  
  
  
## 4. Results

Herewith is a summary of RMSEs as well as the corresponding grades from the 18 models that were built:
  
  
```{r results_grade_output, echo=FALSE, message=FALSE, warning=FALSE}

# function to determine grade from RMSE on validation
grade_rmse <- function(rmse){
  if(is.na(rmse) == TRUE){
    grade <- 0.0
  }else if(rmse > 0.9000000000){
    grade <- 5.0
  }else if(rmse >= 0.8655000000 & rmse <= 0.899990000){
    grade <- 10.0
  }else if(rmse >= 0.8650000000 & rmse <= 0.865490000){
    grade <- 15.0
  }else if(rmse >= 0.8649000000 & rmse <= 0.864990000) {
    grade <- 20.0
  }else if(rmse < 0.8649000000){
    grade <- 25.0
  }
  grade
}

results %>% select(method, RMSE_validation) %>%
  mutate(grade = sapply(RMSE_validation, grade_rmse)) %>%
  knitr::kable()

```
  
  
  
After iterating on the independent variables and determining the best-tuned lambda of 27.5, it could be observed that the "Regularized movie + user model" provides the least RMSE of 0.8571358019 on the validation set.
  
  
  
**Without Regularization:**
  
For the first six models without regularization, the addition of the variables genres, weekday_rated, year_released, and year_rated to movieId and userId failed to effectively reduce the RMSE. Accordingly, the RMSE was pegged to around 0.864 on the validation set as indicated above.
  
  
  
**With Regularization:**
  
Referring to the next six models that were regularized, the addition of both the genres and weekday_rated variables to movieId and userId increased the RMSE from 0.857 to 0.868. To make matters worse, the RMSE increased from 0.857 to around 0.887 when the year_released and year_rated variables were added to the above-mentioned variables.
  
  
  
**Linear Models (lm) and h2o models:**
  
The RMSEs of four h2o models as well as the stats lm() model were in the range of 1.010 to 1.038, which were better than the RMSE of "Just the mean" (naive) model of 1.053.  However, these RMSEs were obviously not better than the RMSEs of either the non-regularized or regularized models which were in the range of 0.857 to 0.941.  This is not to mention the limitation in terms of the intensive compute time required to run the random forests, generalized linear model (glm), deep learning, and gradient boosting machine (gbm) models from the h2o package in processing over 16 million observations. In fact, the time to knit this rmd file would require about 7-8 hours on R 3.5.x Windows 10, 64-bit running on Intel Core i7-7700 with 32 GB RAM.
  
  
  
  
  
  
  
  
## 5. Conclusion

Based of the above results, it is evident that the 'Regularized movie + user' model provides the least RMSE of 0.8571358019 on the validation set. This means that the variables movieId and userId are sufficient to predict the ratings of movies with the least acceptable RMSE. As a limitation, it is likewise evident that the training and implementation of linear model, random forests, gradient boosting, and deep learning models may not be expedient on this type and magnitude of dataset. This is due to the compute-intensiveness and larger memory required to run these models. Perhaps future work could explore and focus on implementing matrix factorization, singular value decomposition (SVD), or principal components analysis (PCA) as described in Section 33.11 of the book (https://rafalab.github.io/dsbook/large-datasets.html#matrix-factorization).  
  
To conclude, it is very possible to reach an RMSE of 0.857 using the regularized movie and user effects.

#######